# importing important packages
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import math
import matplotlib.pyplot as plt

# reading the dataSet
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

#change l1_ratio to 0 except data with penalty equal to 'elasticnet'
def pen(a,b):
    if a == 'elasticnet':
        return b
    else:
        return 0
train_data['l1_ratio']= train_data.apply(lambda x: pen(x.penalty,x.l1_ratio),axis=1)
test_data['l1_ratio']= test_data.apply(lambda x: pen(x.penalty,x.l1_ratio),axis=1)

#one hot train data
train_data=pd.get_dummies(train_data)
test_data=pd.get_dummies(test_data)

#multipy max_iter,n_samples,n_features
def mul_col(a,b,c):
    return a*b*c
train_data['max_iter']=mul_col(train_data['max_iter'],train_data['n_samples'],train_data['n_features'])
test_data['max_iter']=mul_col(test_data['max_iter'],test_data['n_samples'],test_data['n_features'])

#change the value of n_jobs=-1 to 16
train_data['n_jobs']=train_data['n_jobs'].replace([-1],[16])
test_data['n_jobs']=test_data['n_jobs'].replace([-1],[16])

#drop useless features
train_data.drop(['id','n_samples','n_features','random_state','alpha','n_informative','scale'],axis=1,inplace=True)
test_data.drop(['id','n_samples','n_features','random_state','alpha','n_informative','scale'],axis=1,inplace=True)

#get log value f the time
train_data['time']= train_data['time'].apply(lambda x: math.log(x))

# drop time to y_train
x_train=train_data#[train_data['time']<40]  
y_train = x_train['time']
x_train = x_train.drop('time', axis=1)  

#change format
x_train=x_train.values
y_train=y_train.values
test_data=test_data.values

#normalization
mean = x_train.mean(axis=0)
std = x_train.std(axis=0)
x_train = (x_train - mean) / std
test_data = (test_data - mean) / std

#define the CNN model
def build_model():
  model = keras.Sequential([
    keras.layers.Dense(128, activation=tf.nn.relu,
                       input_shape=(x_train.shape[1],)),
    keras.layers.Dense(64, activation=tf.nn.relu),
    keras.layers.Dense(32, activation=tf.nn.relu),
    keras.layers.Dense(1)
  ])
  optimizer = tf.train.RMSPropOptimizer(0.0003)
  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae'])
  return model

# Display training progress by printing a single dot for each completed epoch
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end='')
    
EPOCHS = 500
def plot_history(history):
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [1000$]')
  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),
           label='Train Loss')
  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),
           label = 'Val loss')
  plt.legend()
  plt.ylim([0, 5])
  
model = build_model()
model.summary()
# The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)

history = model.fit(x_train, y_train, epochs=EPOCHS,
                    validation_split=0.2, verbose=0,
                    callbacks=[early_stop, PrintDot()])

plot_history(history)

test_predictions = model.predict(test_data).flatten()
for i in range(100):
    test_predictions[i]=math.exp(test_predictions[i])

